{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ6MDqRXH3g5"
   },
   "source": [
    "# Generalities for GANs\n",
    "\n",
    "* Import libraries\n",
    "* Run on GPU if possible\n",
    "* Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljYSNGg8IHTn",
    "outputId": "1d08ea27-0cea-4a3b-e8ba-af13de0226de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create directory to save results\n",
    "!mkdir results\n",
    "\n",
    "# script parameters\n",
    "batch_size = 128\n",
    "log_interval = 100\n",
    "\n",
    "# run on GPU if possible\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# create data loaders\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-slrIQdIfQ7"
   },
   "source": [
    "# Neural Network class\n",
    "\n",
    "* Generator\n",
    "* Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "rUW-c68rJwRo"
   },
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # declare layers\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        # dummy assignment until this function is filled in\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( 100, 64 * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    # Forward: one ReLU hidden layer of 400 nodes, one Sigmoid output layer of 784 nodes\n",
    "    def forward(self, z):\n",
    "        # dummy assignment until this function is filled in\n",
    "        #return self.main(z)\n",
    "        probabilities = torch.rand(z.shape[0],784).to(device)\n",
    "        return probabilities\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # declare layers\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # dummy assignment until this function is filled in\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Forward: one ReLU hidden layer of 400 nodes, one Sigmoid output layer of 1 node\n",
    "    def forward(self, x):\n",
    "        # dummy assignment until this function is filled in\n",
    "        #return self.main(x)\n",
    "        probabilities = torch.rand(x.shape[0],1).to(device)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1-_ORv-J0j_"
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "Qsc_9JztKEuS"
   },
   "outputs": [],
   "source": [
    "# GAN Loss Function\n",
    "#\n",
    "# Inputs:\n",
    "#    predictions: array of probabilities indicating whether the images are real or fake \n",
    "#    targets: array of 1s (real image) and 0s (fake image)\n",
    "#\n",
    "# Output:\n",
    "#    BCE: binary cross entropy (scalar)\n",
    "#\n",
    "def gan_loss_function(predictions, targets):\n",
    "    # dummy assignment until this function is filled in\n",
    "    BCE = F.binary_cross_entropy(predictions, targets.view(-1, 784), reduction='sum')\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EjxBpPyK5O2"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "wKit3ePwK7eX"
   },
   "outputs": [],
   "source": [
    "# GAN Training\n",
    "#\n",
    "# Have a look at the following tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "#\n",
    "# Inputs:\n",
    "#    epoch: epoch #\n",
    "#    generator: generator network\n",
    "#    generator_optimizer: generator optimizer\n",
    "#    discriminator: discriminator network\n",
    "#    discriminator_optimizer: discriminator optimizer\n",
    "#\n",
    "# Outputs:\n",
    "#    average_generator_loss: binary cross entropy (scalar)\n",
    "#    average_discriminator_loss: binary cross entropy (scalar)\n",
    "#\n",
    "def gan_train(epoch, generator, generator_optimizer, discriminator, discriminator_optimizer):\n",
    "    # dummy assignment until this function is filled in\n",
    "    average_discriminator_loss = 0\n",
    "    average_generator_loss = 0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        discriminator.zero_grad()\n",
    "        real_cpu = data.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
    "        output = discriminator(real_cpu)\n",
    "        errD_real = gan_loss_function(output, real_cpu)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        noise = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "        fake = generator(noise)\n",
    "        label.fill_(0)\n",
    "        output = discriminator(fake.detach()).view(-1)\n",
    "        errD_fake = gan_loss_function(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        discriminator_optimizer.step()\n",
    "        generator.zero_grad()\n",
    "        label.fill_(1)\n",
    "        output = discriminator(fake).view(-1)\n",
    "        errG = gan_loss_function(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        generator_optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        average_discriminator_loss += errD.item()\n",
    "        average_generator_loss += errG.item()\n",
    "    average_discriminator_loss /= len(train_loader.dataset)\n",
    "    average_generator_loss /= len(train_loader.dataset)\n",
    "    return average_generator_loss, average_discriminator_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCKL-oZRL88a"
   },
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "8ocAuSUyMIvc"
   },
   "outputs": [],
   "source": [
    "# GAN Test\n",
    "#\n",
    "# Have a look at the following tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "#\n",
    "# Inputs:\n",
    "#    epoch: epoch #\n",
    "#    generator: generator network\n",
    "#    discriminator: discriminator network\n",
    "#\n",
    "# Outputs:\n",
    "#    average_generator_loss: binary cross entropy (scalar)\n",
    "#    average_discriminator_loss: binary cross entropy (scalar)\n",
    "#\n",
    "def gan_test(epoch, generator, discriminator):\n",
    "    # dummy assignment until this function is filled in\n",
    "    average_generator_loss = 0\n",
    "    average_discriminator_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            real_cpu = data.to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), 1, dtype=torch.float, device=device)\n",
    "            output = discriminator(real_cpu).view(-1)\n",
    "            average_discriminator_loss += gan_loss_function(output, label).item()\n",
    "            noise = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "            fake = generator(noise)\n",
    "            label.fill_(0)\n",
    "            output = discriminator(fake.detach()).view(-1)\n",
    "            average_discriminator_loss += gan_loss_function(output, label).item()\n",
    "            label.fill_(1)\n",
    "            output = discriminator(fake).view(-1)\n",
    "            average_generator_loss += gan_loss_function(output, label).item()\n",
    "    average_discriminator_loss /= len(test_loader.dataset)\n",
    "    average_generator_loss /= len(test_loader.dataset)     \n",
    "    return average_generator_loss, average_discriminator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwdEzswjNO--"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "bGQp6JTfPKUL"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [125]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m discriminator_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(discriminator_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     generator_average_train_loss, discriminator_average_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgan_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     discriminator_average_train_losses\u001b[38;5;241m.\u001b[39mappend(discriminator_average_train_loss)\n\u001b[1;32m     14\u001b[0m     generator_average_train_losses\u001b[38;5;241m.\u001b[39mappend(generator_average_train_loss)\n",
      "Input \u001b[0;32mIn [123]\u001b[0m, in \u001b[0;36mgan_train\u001b[0;34m(epoch, generator, generator_optimizer, discriminator, discriminator_optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b_size,), \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m output \u001b[38;5;241m=\u001b[39m discriminator(real_cpu)\n\u001b[0;32m---> 26\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m \u001b[43mgan_loss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m errD_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m D_x \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[0;32mIn [119]\u001b[0m, in \u001b[0;36mgan_loss_function\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgan_loss_function\u001b[39m(predictions, targets):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# dummy assignment until this function is filled in\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     BCE \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BCE\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2906\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2904\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 2906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2909\u001b[0m     )\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2912\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# train and test gan\n",
    "epochs = 50\n",
    "discriminator_average_train_losses = []\n",
    "discriminator_average_test_losses = []\n",
    "generator_average_train_losses = []\n",
    "generator_average_test_losses = []\n",
    "generator_model = Generator(0).to(device)\n",
    "generator_optimizer = optim.Adam(generator_model.parameters(), lr=1e-3)\n",
    "discriminator_model = Discriminator(0).to(device)\n",
    "discriminator_optimizer = optim.Adam(discriminator_model.parameters(), lr=1e-3)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    generator_average_train_loss, discriminator_average_train_loss = gan_train(epoch, generator_model, generator_optimizer, discriminator_model, discriminator_optimizer)\n",
    "    discriminator_average_train_losses.append(discriminator_average_train_loss)\n",
    "    generator_average_train_losses.append(generator_average_train_loss)\n",
    "    generator_average_test_loss, discriminator_average_test_loss = gan_test(epoch, generator_model, discriminator_model)\n",
    "    discriminator_average_test_losses.append(discriminator_average_test_loss)\n",
    "    generator_average_test_losses.append(generator_average_test_loss)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = generator_model(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')\n",
    "        print('Sample of generated images')\n",
    "        display(Image('results/sample_' + str(epoch) + '.png'))\n",
    "        print('\\n')\n",
    "\n",
    "# Plot Train losses\n",
    "plt.plot(discriminator_average_train_losses)\n",
    "plt.plot(generator_average_train_losses)\n",
    "plt.title('Train BCE Losses')\n",
    "plt.ylabel('Binary Cross Entropy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.legend(['Discriminator','Generator'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot Test Losses\n",
    "plt.plot(discriminator_average_test_losses)\n",
    "plt.plot(generator_average_test_losses)\n",
    "plt.title('Test BCE Losses')\n",
    "plt.ylabel('Binary Cross Entropy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.legend(['Discriminator','Generator'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cs480_fall20_asst6_gan_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
